{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPuJK9iQvLZq0sXijh6A6mg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bapanichhua/Data-Structure-Assignment/blob/main/Untitled23.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Theory Answers:-\n",
        "\n",
        "\n",
        "#1.What is Logistic Regression, and how does it differ from Linear Regression?\n",
        "Logistic Regression is a classification algorithm used to predict categorical outcomes. It models the probability that a given input belongs to a particular class using the sigmoid function. Unlike Linear Regression, which outputs continuous values, Logistic Regression outputs probabilities and maps them to class labels.\n",
        "\n",
        "#2. What is the mathematical equation of Logistic Regression?\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "𝑒\n",
        "−\n",
        "(\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑥\n",
        "1\n",
        "+\n",
        "𝛽\n",
        "2\n",
        "𝑥\n",
        "2\n",
        "+\n",
        ".\n",
        ".\n",
        ".\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑥\n",
        "𝑛\n",
        ")\n",
        "P(y=1∣x)=\n",
        "1+e\n",
        "−(β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " x\n",
        "1\n",
        "​\n",
        " +β\n",
        "2\n",
        "​\n",
        " x\n",
        "2\n",
        "​\n",
        " +...+β\n",
        "n\n",
        "​\n",
        " x\n",
        "n\n",
        "​\n",
        " )\n",
        "\n",
        "1\n",
        "​\n",
        "\n",
        "# 3.Why do we use the Sigmoid function in Logistic Regression?\n",
        "The sigmoid function maps any real-valued number to the (0, 1) range, making it suitable for modeling probabilities.\n",
        "\n",
        "#4. What is the cost function of Logistic Regression?\n",
        "The log loss (binary cross-entropy) function:\n",
        "\n",
        "𝐽\n",
        "(\n",
        "𝜃\n",
        ")\n",
        "=\n",
        "−\n",
        "1\n",
        "𝑚\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑚\n",
        "[\n",
        "𝑦\n",
        "(\n",
        "𝑖\n",
        ")\n",
        "log\n",
        "⁡\n",
        "(\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        ")\n",
        "+\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑦\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "log\n",
        "⁡\n",
        "(\n",
        "1\n",
        "−\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        ")\n",
        "]\n",
        "J(θ)=−\n",
        "m\n",
        "1\n",
        "​\n",
        "  \n",
        "i=1\n",
        "∑\n",
        "m\n",
        "​\n",
        " [y\n",
        "(i)\n",
        " log(h\n",
        "θ\n",
        "​\n",
        " (x\n",
        "(i)\n",
        " ))+(1−y\n",
        "(i)\n",
        " )log(1−h\n",
        "θ\n",
        "​\n",
        " (x\n",
        "(i)\n",
        " ))]\n",
        "#5.What is Regularization in Logistic Regression? Why is it needed?\n",
        "Regularization adds a penalty to the loss function to reduce model complexity and prevent overfitting. It discourages large weights.\n",
        "\n",
        "#6.Explain the difference between Lasso, Ridge, and Elastic Net regression.\n",
        "\n",
        "Lasso (L1): Adds absolute value of coefficients. Performs feature selection.\n",
        "\n",
        "Ridge (L2): Adds squared values of coefficients. Shrinks coefficients but doesn’t eliminate.\n",
        "\n",
        "Elastic Net: Combines L1 and L2. Useful when features are correlated.\n",
        "\n",
        "#7.When should we use Elastic Net instead of Lasso or Ridge?\n",
        "Use Elastic Net when there are multiple correlated features or when you want both feature selection and regularization.\n",
        "\n",
        "#8.What is the impact of the regularization parameter (λ) in Logistic Regression?\n",
        "λ controls the strength of regularization. A higher λ means more penalty, leading to smaller coefficients and potentially underfitting.\n",
        "\n",
        "#9.What are the key assumptions of Logistic Regression?\n",
        "\n",
        "Linear relationship between independent variables and the log-odds.\n",
        "\n",
        "No multicollinearity.\n",
        "\n",
        "Large sample size.\n",
        "\n",
        "Independent observations.\n",
        "\n",
        "#10.What are some alternatives to Logistic Regression for classification tasks?\n",
        "\n",
        "Decision Trees\n",
        "\n",
        "Random Forest\n",
        "\n",
        "SVM\n",
        "\n",
        "k-NN\n",
        "\n",
        "Naive Bayes\n",
        "\n",
        "Neural Networks\n",
        "\n",
        "#11.What are Classification Evaluation Metrics?\n",
        "\n",
        "Accuracy\n",
        "\n",
        "Precision\n",
        "\n",
        "Recall\n",
        "\n",
        "F1-Score\n",
        "\n",
        "ROC-AUC\n",
        "\n",
        "Confusion Matrix\n",
        "\n",
        "#12.How does class imbalance affect Logistic Regression?\n",
        "It can bias the model towards the majority class. Use techniques like class weights, resampling, or SMOTE to address this.\n",
        "\n",
        "#13.What is Hyperparameter Tuning in Logistic Regression?\n",
        "The process of selecting the best values for parameters like C, penalty, and solver using techniques like GridSearchCV or RandomizedSearchCV.\n",
        "\n",
        "#13.What are different solvers in Logistic Regression? Which one should be used?\n",
        "\n",
        "liblinear: Good for small datasets and binary classification. Supports L1.\n",
        "\n",
        "lbfgs: Good for multiclass problems and large datasets. Supports L2.\n",
        "\n",
        "sag and saga: For large datasets. saga supports Elastic Net.\n",
        "\n",
        "#14.How is Logistic Regression extended for multiclass classification?\n",
        "\n",
        "One-vs-Rest (OvR)\n",
        "\n",
        "Softmax Regression (multinomial)\n",
        "\n",
        "#15.What are the advantages and disadvantages of Logistic Regression?\n",
        "Advantages: Simple, interpretable, fast, works well with linearly separable data.\n",
        "Disadvantages: Poor performance with non-linear problems, sensitive to outliers, assumes linear decision boundary.\n",
        "\n",
        "#16.What are some use cases of Logistic Regression?\n",
        "\n",
        "Email spam detection\n",
        "\n",
        "Disease prediction\n",
        "\n",
        "Credit scoring\n",
        "\n",
        "Marketing campaign targeting\n",
        "\n",
        "#17.What is the difference between Softmax Regression and Logistic Regression?\n",
        "Logistic Regression is binary, while Softmax Regression generalizes it to multiclass problems by computing probabilities for all classes.\n",
        "\n",
        "#18.How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?\n",
        "\n",
        "Use OvR for interpretability or when using binary solvers.\n",
        "\n",
        "Use Softmax for better performance on truly multiclass problems.\n",
        "\n",
        "#19.How do we interpret coefficients in Logistic Regression?\n",
        "Each coefficient represents the change in the log-odds of the outcome per unit increase in the predictor.\n",
        "\n",
        "Odds Ratio\n",
        "=\n",
        "𝑒\n",
        "𝛽\n",
        "𝑖\n",
        "Odds Ratio=e\n",
        "β\n",
        "i\n",
        "​\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#\n",
        "Practical Answers:-\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "q-_rz3lVFffr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and prints the model accuracy ?\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIluOj-TGzEy",
        "outputId": "f4db2af3-8286-40ff-98e1-8d3daf209bda"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2.Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1') and print the model accuracy ?\n",
        "model = LogisticRegression(penalty='l1', solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"L1 Regularized Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69FEni93G_cm",
        "outputId": "94c2c3be-345c-4d37-8c4e-3cf8100bfd53"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L1 Regularized Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.Write a Python program to train Logistic Regression with L2 regularization (Ridge) using LogisticRegression(penalty='l2'). Print model accuracy and coefficients ?\n",
        "\n",
        "model = LogisticRegression(penalty='l2', solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "print(\"L2 Accuracy:\", accuracy_score(y_test, model.predict(X_test)))\n",
        "print(\"Coefficients:\", model.coef_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHgAnxV8HUP7",
        "outputId": "b142894b-96af-45bb-9bef-947525dc5c2b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L2 Accuracy: 1.0\n",
            "Coefficients: [[ 0.3711229   1.409712   -2.15210117 -0.95474179]\n",
            " [ 0.49400451 -1.58897112  0.43717015 -1.11187838]\n",
            " [-1.55895271 -1.58893375  2.39874554  2.15556209]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4. Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet')?\n",
        "model = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Elastic Net Accuracy:\", accuracy_score(y_test, model.predict(X_test)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idudxL0THgmq",
        "outputId": "6d7c6738-d29a-42b9-9d14-e3d8e4992108"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elastic Net Accuracy: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5.Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr' ?\n",
        "model = LogisticRegression(multi_class='ovr', solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "print(\"OvR Accuracy:\", accuracy_score(y_test, model.predict(X_test)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmHX464JHrcr",
        "outputId": "eb10b6b9-0d05-47a5-957a-e3a23e49732e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OvR Accuracy: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#6.Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression. Print the best parameters and accuracy ?\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "params = {\n",
        "    'C': [0.01, 0.1, 1, 10],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear']\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(LogisticRegression(), param_grid=params, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Accuracy:\", grid.best_score_)\n",
        "0."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8QlOBo2H69R",
        "outputId": "49c11716-fb57-4dda-ac3c-e39e730827af"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 10, 'penalty': 'l1', 'solver': 'liblinear'}\n",
            "Best Accuracy: 0.9583333333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7.Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the average accuracy?\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5)\n",
        "model = LogisticRegression(max_iter=200)\n",
        "scores = cross_val_score(model, X, y, cv=skf)\n",
        "print(\"Stratified CV Avg Accuracy:\", scores.mean())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKgJ4pULIn5_",
        "outputId": "c28e7360-c058-4b1b-ac4b-59f18bba7bd2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stratified CV Avg Accuracy: 0.9733333333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#8.Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy ?\n",
        "import pandas as pd\n",
        "\n",
        "# Replace 'data.csv' with your dataset path\n",
        "df = pd.read_csv(\"data.csv\")\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "print(\"CSV Data Accuracy:\", accuracy_score(y_test, model.predict(X_test)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "9hRbIBsRI3DE",
        "outputId": "6b1617a9-fb62-4f1d-e2f3-68cc43a4d1b7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'data.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-188fe2105c54>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Replace 'data.csv' with your dataset path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#9.Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in Logistic Regression. Print the best parameters and accuracy ?\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "param_dist = {\n",
        "    'C': [0.01, 0.1, 1, 10],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear', 'saga']\n",
        "}\n",
        "\n",
        "random_search = RandomizedSearchCV(LogisticRegression(max_iter=500), param_distributions=param_dist, cv=5, n_iter=10)\n",
        "random_search.fit(X_train, y_train)\n",
        "print(\"Best Params:\", random_search.best_params_)\n",
        "print(\"Best Accuracy:\", random_search.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "posbSgMgJFEC",
        "outputId": "d7fc1237-3e75-46c7-b4e4-c0a7ebaae7d9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Params: {'solver': 'saga', 'penalty': 'l1', 'C': 1}\n",
            "Best Accuracy: 0.975\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#10.Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy ?\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "\n",
        "ovo_model = OneVsOneClassifier(LogisticRegression(max_iter=200))\n",
        "ovo_model.fit(X_train, y_train)\n",
        "print(\"OvO Accuracy:\", accuracy_score(y_test, ovo_model.predict(X_test)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlahIDgoJcpS",
        "outputId": "a5184572-b944-4f50-c491-eca7bfba4dcc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OvO Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#11.Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary classification ?\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Assume binary classification dataset\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "hcKMzLE4Jp8K",
        "outputId": "945a6312-d9df-4306-c4a2-26533e1164a8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7b7f377a87d0>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#12.Write a Python program to train a Logistic Regression model and evaluate its performance using Precision, Recall, and F1-Score ?\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "print(\"Precision:\", precision_score(y_test, y_pred, average='macro'))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred, average='macro'))\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred, average='macro'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHaRbWnxJ2MA",
        "outputId": "4c96d842-bb22-4302-8988-e83255c9d0ef"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1 Score: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#13.Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to improve model performance ?\n",
        "model = LogisticRegression(class_weight='balanced')\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Accuracy with class weights:\", accuracy_score(y_test, model.predict(X_test)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amdfbiZ_KLRN",
        "outputId": "bc96b436-2ec2-4545-d47e-08400731f65b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with class weights: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#14.Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and evaluate performance ?\n",
        "import seaborn as sns\n",
        "\n",
        "titanic = sns.load_dataset(\"titanic\")\n",
        "titanic.dropna(subset=['age', 'fare', 'embarked', 'sex'], inplace=True)\n",
        "\n",
        "# Encode and prepare\n",
        "X = pd.get_dummies(titanic[['age', 'fare', 'embarked', 'sex']], drop_first=True)\n",
        "y = titanic['survived']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Titanic Accuracy:\", accuracy_score(y_test, model.predict(X_test)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gX5SjiVLKRiB",
        "outputId": "00dbbc1d-e4e7-4b27-ea56-67123ca059b4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Titanic Accuracy: 0.7552447552447552\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#15. Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression model. Evaluate its accuracy and compare results with and without scaling ?\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2)\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Accuracy with scaling:\", accuracy_score(y_test, model.predict(X_test)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i98OHmZtKaBO",
        "outputId": "14e384ff-bb55-49bf-91f0-720e092c250f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with scaling: 0.7692307692307693\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#16.Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score ?\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "y_proba = model.predict_proba(X_test)[:, 1]\n",
        "print(\"ROC-AUC:\", roc_auc_score(y_test, y_proba))\n"
      ],
      "metadata": {
        "id": "0EZUTNPjb7m5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#17.Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate accuracy?\n",
        "model = LogisticRegression(C=0.5)\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Custom C Accuracy:\", accuracy_score(y_test, model.predict(X_test)))\n"
      ],
      "metadata": {
        "id": "0AcZKVGTcDTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#18.Write a Python program to train Logistic Regression and identify important features based on model coefficients ?\n",
        "import numpy as np\n",
        "\n",
        "coefs = model.coef_[0]\n",
        "feature_importance = pd.Series(coefs, index=X.columns)\n",
        "print(\"Top features:\\n\", feature_importance.sort_values(ascending=False))\n"
      ],
      "metadata": {
        "id": "MHIoT7bvcSEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#19.Write a Python program to train Logistic Regression and evaluate its performance using Cohen’s Kappa Score ?\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "print(\"Cohen’s Kappa:\", cohen_kappa_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "s0izbn8LcdOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#20.M Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary classificatio ?\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
        "plt.plot(recall, precision)\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precision-Recall Curve\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ca9ICFJIcpyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#21.Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare their accuracy ?\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "for solver in solvers:\n",
        "    model = LogisticRegression(solver=solver, max_iter=500)\n",
        "    model.fit(X_train, y_train)\n",
        "    acc = accuracy_score(y_test, model.predict(X_test))\n",
        "    print(f\"Solver: {solver}, Accuracy: {acc}\")\n"
      ],
      "metadata": {
        "id": "uojWeAGrcz9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#22.Write a Python program to train Logistic Regression and evaluate its performance using Matthews Correlation Coefficient (MCC) ?\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "print(\"MCC:\", matthews_corrcoef(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "F5SdW7kac9F_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#23.Write a Python program to train Logistic Regression on both raw and standardized data. Compare their accuracy to see the impact of feature scaling ?\n",
        "# Raw\n",
        "model_raw = LogisticRegression(max_iter=200)\n",
        "model_raw.fit(X_train, y_train)\n",
        "acc_raw = accuracy_score(y_test, model_raw.predict(X_test))\n",
        "\n",
        "# Standardized\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_train_s, X_test_s, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2)\n",
        "model_std = LogisticRegression(max_iter=200)\n",
        "model_std.fit(X_train_s, y_train)\n",
        "acc_std = accuracy_score(y_test, model_std.predict(X_test_s))\n",
        "\n",
        "print(\"Raw Accuracy:\", acc_raw)\n",
        "print(\"Standardized Accuracy:\", acc_std)\n"
      ],
      "metadata": {
        "id": "qnFhF-hCdFd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#24.Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using cross-validation ?\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "for c in [0.01, 0.1, 1, 10]:\n",
        "    model = LogisticRegression(C=c, max_iter=200)\n",
        "    scores = cross_val_score(model, X, y, cv=5)\n",
        "    print(f\"C={c}, Mean Accuracy={scores.mean()}\")\n"
      ],
      "metadata": {
        "id": "iuk2C8SNdQii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#25.Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to make predictions. ?\n",
        "import joblib\n",
        "\n",
        "# Train and save\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "joblib.dump(model, 'logistic_model.pkl')\n",
        "\n",
        "# Load and predict\n",
        "loaded_model = joblib.load('logistic_model.pkl')\n",
        "print(\"Loaded Model Accuracy:\", accuracy_score(y_test, loaded_model.predict(X_test)))\n"
      ],
      "metadata": {
        "id": "weQVDW21dZr-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}